import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix,classification_report
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score

dataset = pd.read_csv('https://raw.githubusercontent.com/Denekewem/Excel/main/mois_all.csv')
len(dataset)
dataset.head()
# Replace zeroes
zero_not_accepted = ['Moisture_value']
for column in zero_not_accepted:
    dataset[column] = dataset[column].replace(0, np.NaN)
    mean = int(dataset[column].mean(skipna=True))
    dataset[column] = dataset[column].replace(np.NaN, mean)
    # split dataset
X = dataset.iloc[:, 0:1]
y = dataset.iloc[:, 1]
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.5)
print(len(X_train))
print(len(y_train))
print(len(X_test))
print(len(y_test))
#Feature scaling
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)
# Define the model: Init K-NN
classifier = KNeighborsClassifier(n_neighbors=5, p=2,metric='euclidean')
# Fit Model
classifier.fit(X_train, y_train)
# Predict the test set results
y_pred = classifier.predict(X_test)
y_pred
# Evaluate Model
cm = confusion_matrix(y_test, y_pred)
print (cm)
print(classification_report(y_test, y_pred))
print(round(accuracy_score(y_test, y_pred)*100,2))

#print(cmat)
print('TP - True Negative {}'.format(cm[0,0]))
print('FP - False Positive {}'.format(cm[0,1]))
print('FN - False Negative {}'.format(cm[1,0]))
print('TP - True Positive {}'.format(cm[1,1]))
print('Accuracy Rate: {}'.format(np.divide(np.sum([cm[0,0],cm[1,1]]),np.sum(cm))))
print('Misclassification Rate: {}'.format(np.divide(np.sum([cm[0,1],cm[1,0]]),np.sum(cm))))

# Generate function to add error rates of KNN with various k-values
# error_rate -> empty list to gather error rates at various k-values
# for loop -> loops through k values 1 to 39
# knn -> creates instance of KNeighborsClassifier with various k
# knn.fit -> trains the model
# pred_i -> conducts predictions from model on test subset
# error_rate.append -> adds error rate of model with various k-value, using the average where prediction not
# equal to the test values
error_rate = []
for i in range(1,40):
    
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train, y_train)
    pred_i = knn.predict(X_test)
    error_rate.append(np.mean(pred_i != y_test))
    
    # Configure and plot error rate over k values
plt.figure(figsize=(10,4))
plt.plot(range(1,40), error_rate, color='blue', linestyle='dashed', marker='o', markerfacecolor='red', markersize=10)
plt.title('Error Rate vs. K-Values')
plt.xlabel('K-Values')
plt.ylabel('Error Rate')

# Retrain model using optimal k-value
# Print out classification report and confusion matrix

